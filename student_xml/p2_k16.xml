<?xml version="1.0" encoding="UTF-8"?>
<?xml-model href="../schema_3302.rng" type="application/xml" schematypens="http://relaxng.org/ns/structure/1.0"?>
<!--the second line in the document associates the schema, so be sure not to change it-->
<DOC>
    <docHead>
        <!--required header includes metadata about the assignment (title, author, version)-->
        <title>Project 2</title>
        <author xml:id="your_id_here">Gurtaj Khatra</author>
        <version n="num_of_version" date="2016-08-01"/>
    </docHead>

    <blog_post><note type="instructor" resp="#kgs">I'm surprised there's no section header to
            start...</note>
        <background>
            <pb/> If you have read any technology news or blogs in the past few years, you probably
            have heard of something called “machine learning” being thrown around constantly.
            Everybody seems to be using it now: car companies are using <note type="instructor"
                resp="#kgs">it</note>to make self driving cars, phones companies are using it to
            make digital assistants that can understand what you're saying, and social media
            websites are using it to detect your face in your friends pictures. However, even with
            all of these useful applications of machine learning, most people have no idea
                whats<note type="instructor" resp="#kgs">what's</note> going on behind the scenes to
            make all of this<note type="instructor" resp="#kgs">what is 'this' here--all of these
                different applications of ML?</note> work. <technical_principle>If you skim through
                a book on machine learning, you will probably see a lot of complicated looking math
                and statistics,<misconception>which makes most people think that it’s something that
                    would require an advanced degree to understand, but the idea behind it really
                    isn’t that complicated.</misconception></technical_principle>
            <question>So how exactly does this thing called “machine learning” really work?
            </question>
        </background>
        <answer>
            <section_header>Is machine learning just black magic? </section_header>
            <technical_principle> At it’s heart, machine learning is just statistics and
                probability. <analogy>From a statistical perspective, it works in the same way a
                    professional poker player would look at the cards in their hand and what is on
                    table, and then try to make a decision on what move they should make
                    next.</analogy> But we have had mathematicians working on and developing
                statistical ideas for centuries now, which leads to the question: </technical_principle>
            <section_header> Why has machine learning only been becoming popular in the past few
                years? </section_header>
            <technical_principle> Outside of mathematical theory, we haven’t been able to do much
                with statistics until computers came around. When computers first started being used
                commonly a few decades ago, there were a few people trying to build these
                statistical models and do some machine learning but it was extremely slow and
                inefficient, even with the most powerful computers at the time. However with huge
                advances in computing power over the past few years, being able to create these
                statistical models became much faster, accessible, and reliable. </technical_principle>
            <section_header>What kind of statistics does machine learning use?</section_header>
            <specific_topic>For this example, we are just going to explain how classification by
                logistic regression in machine learning works.<note type="instructor" resp="#kgs"
                    >why are we only focusing on this aspect? What advantages does this approach
                    have?</note> Classification is just a way to label a set of information, such as
                labeling a set of pictures of dogs by the breed of dog in each photo. All this
                specific machine learning algortihm does is generate a math equation that looks like
                    this:<visual type="drawing" url="http://i.stack.imgur.com/5vpYa.png">Logistic
                    Regression</visual>Let says our dog photos only have pictures of poodles and
                huskies. The machine learning algorithm will take a photo, run it through another
                generated equation which would spit out a number, and it finds this number on the x
                axis and calculates the corresponding value on the y axis. Since we are only trying
                to classify two differnt<note type="instructor" resp="#kgs">different</note> types
                of dogs, there are only two cases we care about: <list type="generic"> 1. The y
                    value is between 0.5 and 1, in which case we can say we have a husky in the
                    photo. <pb/> 2. The y value is between 0 and 0.5, in which case we can say we
                    have a poodle in the photo. <pb/>
                </list>
                <pb/>
            </specific_topic>
            <section_header>But how do we come up with all of these equations?</section_header>
            <explanation n="1"> This is where the “learning” part of machine learning comes in. In
                the example we had an equation that could tell the difference between a poodle and a
                husky, but to come up with the equation, we needed to “teach” the computer how to
                create it. To do this, we need to give a computer a training set of already labeled
                dog photos. It can go through the photos and identify differing features between
                poodles and huskies, such as pointless of it’s ears. The way it identifies these
                features is by using something called a “convolutional neural network”, but that is
                another whole can of worms (you can read more about them in <ref type="supplement"
                    url="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721#.v1l1nui91"
                    >a blog post here</ref> if you're interested). Now since we want the
                classification equation to output a number between 0 and 0.5 if we have a poodle and
                0.5 and 1 if we have a husky, the computer will form the equation in a way so that
                the pointier the ears are, the higher number the equation will output. We can see
                approximately where some dog photos would be plotted in the following graph using
                our newly trained equation: <pb/>
                <visual type="drawing" url="http://i.imgur.com/iO8lFWg.png"/> Since the difference
                between huskies and poodles is more than just pointy ears, we can create multiple
                equations that look at different features such as tail shape, number of fur colors,
                or eye shape and use all of them to create a better and more accurate classifier. </explanation>
            <conclusion>If you look past all of the complex math and statistics involved in machine
                learning, it really is just the computer giving its best guess. Although it is far
                from perfect, with the amount of research being done in the field and the pace at
                which computing power is advancing, machine learning is becoming more and more
                accurate and more prevalent in our everyday lives. </conclusion>
        </answer>
        <sources>
            <citation style="MLA">Brownlee, Jason. "Logistic Regression for Machine Learning -
                Machine Learning Mastery." Machine Learning Mastery. N.p., 1 Apr. 2016. Web. 01 Aug.
                2016.</citation>
        </sources>
    </blog_post>

    <!--paste body of your template file here-->
    <docReview>
        <reviewer type="peer" xml:id="g14">Ben Gincley</reviewer>
        <report type="peer" resp="#g14">
            <p>This document makes use of the science blog format to describe the fundamentals
                behind machine learning. Through the use of analogy, example, and images, it breaks
                down the more complicated math and probability involved giving a cursory overview of
                the topic. </p>
            <p>One brief note on formatting, I have found it helpful to wrap images in paragraph
                breaks to isolate them from the text of the document, thus eliminating the weird
                hanging line effect. The blog makes great use of brevity to briefly highlight key
                points without getting bogged down in the weeds of a particular aspect of a topic.
                However, the mention of a "neural network" at the end of the piece felt a little too
                brief, and that there was something lacking. One or two sentences to add a little
                more information on the topic would help the choppiness felt when reading (here is a
                topic, this is a discussion for another time, now let's proceed in a different
                direction). Overall it felt like a sudden redirection of flow that was a little
                disorienting. Additionally, perhaps further on this note, the level of explanation
                felt a little lacking on the "learning" aspect of the topic towards the end. While
                this is more of a content critique than a schema/structural one, I felt that when
                reading, Gurtaj did a fantastic job explaining what the machine was doing, but was a
                little brief on the "learning" component, using primarily one sentence about feeding
                the machines examples of already labeled images to "train" them. More information on
                this "training" (a sentence or two) would help convey this aspect a little better. </p>
            <p>Overall, this post did a great job using the devices/elements of the genre to break
                down a complicated topic into a simple explanation. Just looking at the XML itself,
                it looks like the "question_relevant_topic" is missing, so tagging a portion of the
                answer with this element should resolve that issue. Also, the "misconceptions"
                element is embedded in the "scientific_principles" element, when to appropriately
                abide the schema I believe they need to be independent. Omission of a few elements
                like the "road_map" seem appropriate given the brevity of the article on the whole.
                Aside from a few grammatical errors here and there, this draft did a swell job at
                responding to the prompt and abiding by the genre conventions. The diction is
                appropriately common vernacular for most of the text, and technical terms are
                explained using simpler words. After a thorough read-through to spell and grammar
                check, and a few amendments to the XML tagging, and a little more detail in the last
                leg of the answer, this should stand up as a great example of the genre. </p>
        </report>
        <note type="peer" resp="#k_16"> *QUESTIONS ON BOARD* 1)What is a going well? Overall, I feel
            confident in the schema my group devised for our scientific blog post. It feels like it
            fits the criteria of the science blogs we studied, but it is still fairly flexible and
            was intuitive to create my own post. I also feel like a picked a topic that I’m pretty
            interested in, and have a decent amount of knowledge about. 2) What has been difficult
            so far? Writing the actual post was a little difficult. The topic I chose for my
            scientific blog post was “machine learning” which can be a broad topic. Keeping my post
            concise as well as easily digestible was a task that took a decent amount of time to
            accomplish. 3) Where are you in your process? Right now I have a rough draft of my post
            that fits the schema we made. 4) Specific things: Skimming through it just now, I feel
            like there are parts that I can take out, and parts where I should expand more on my
            topic. I feel like I should add a few more elements such as analogies or diagrams to
            help explain my topic. </note>
        <reviewer type="instructor" xml:id="kgs"/>
        <report type="instructor" resp="#kgs">
            <p>There is a lot going well here. I think your style and tone are very appropriate,
                your use of images is well done, and the primary analogy and secondary anaologies
                you deploy are helpful for a novice (but interested) reader to gain a cursory
                understanding. A few issues I noticed: there are some simple editing errors I tried
                to indicate; I think a better sense of *why* you focus on classification (perhaps
                that relates to your intro about the wide arroy of applications of machine learning;
                lastly, the post ended rather abruptly--I think a more developed conclusion that
                points back to your introduction would be helpful for a reader: what do we now know
                about how people use ML? What further reading can we do if we are still interested?
                How would one go about applying classification to the ML you describe in the
                background? Etc. Overall, though, I think this is a good example of the genre and
                certainly the topic is perfect for this treatment.</p>
        </report>
    </docReview>
</DOC>
